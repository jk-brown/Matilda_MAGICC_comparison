---
title: "03-Running Hector with NDC emissions input"
author: "Joe Brown"
date: "2024-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

The goal of this document is to run Hector with Matilda using NDC emissions scenarios to compare with results from MAGICC7 used in Ou and Iyer 2021.

## Build list of input files

Build input file list of all the NDC experimental scenarios:

```{r, list files in ini directory}
input_file_directory <- "data/input/ini/"

input_files <- list.files(input_file_directory)

input_file_list <- list(T_01_BAU = paste0(input_file_directory, input_files[1]),
                    T_02_CAT_crnt = paste0(input_file_directory, input_files[2]), 
                    T_03_CAT_crnt = paste0(input_file_directory, input_files[3]),
                    T_04_NDC_cont = paste0(input_file_directory, input_files[4]), 
                    T_05_NDC_inc_LTS = paste0(input_file_directory, input_files[5]), 
                    T_06_NDC_cont = paste0(input_file_directory, input_files[6]),
                    T_07_NDC_inc_LTS = paste0(input_file_directory, input_files[7]))

```

## Matilda workflow

Use the NDC ini list to run Hector with the Matilda workflow.

### Build parameter set

```{r, construct perturbed parameter set}
# set seed for replication
set.seed(123)

# set sample size for param generation
n = 1000

# initiate a core that can be used to generate parameters -- here we will use SSP2-4.5
param_core <- newcore(system.file("input/hector_ssp245.ini", package = "hector"))

# generate param set
params <- generate_params(core = param_core,
                          draws = n)

```

### Split jobs for parallel processing

```{r, split params into chunks and prepare cluster}
# splitting params df into chunks
param_chunks <- split(params, 1:10)

# prep cluster
# initiate a cluster
cluster <- makeCluster(detectCores() - 1)

# export functions and objects to the core
clusterExport(cluster, c("input_file_list", 
                         "param_chunks", 
                         "newcore", 
                         "reset", 
                         "iterate_model"))

```

### Run the model
```{r, run the model with each ini file}

# run the model 
time_start = Sys.time()

model_result <- parLapply(cluster, names(input_file_list), function(scenario_name){

  # extract scenario information 
  scenario <- input_file_list[[scenario_name]]
  
  # initialize a model core for the current scenario
  core <- newcore(scenario, name = scenario_name)
  
  # run the model looking across param_chunks 
  result_list <- lapply(param_chunks, function(chunk) {
    
    iterate_model(core = core, 
                  params = chunk,
                  save_years = 1800:2100,
                  save_vars = c("gmst",
                                "CO2_concentration",
                                "global_tas",
                                "ocean_uptake"))
  })
  
  # reset core for each iteration
  reset(core)
  
  # Convert run_number to continuous among chunks 
  for (i in 2:length(result_list)) {
    
    # calculate the max run_number of the previous element in result_list
    max_run_number <- max(result_list[[i-1]]$run_number)
    
    # Add the max run_number of the previous element to the run_number of the current element
    result_list[[i]]$run_number <- result_list[[i]]$run_number + max_run_number
    
  }
  
  bind_result <- do.call(rbind, result_list)
  
  return(bind_result)
  
})

time_end = Sys.time() - time_start

```


Save the result:
```{r}
# save result as .RDS file
saveRDS(model_result, "data/output/model_result.RDS")

```

### Score model runs

```{r score model runs}


```

